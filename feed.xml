<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>苏苏</title>
    <link href="" rel="self" />
    <link href="http://sukai.me/" />
    <updated>2015-12-07T12:00:00Z</updated>
    <id>http://sukai.me/</id>
    
    <entry>
        <title><![CDATA[Machine Learnging应用的几处建议]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/advice-for-applying-ml-svm/"/>
        <updated>2015-12-07T12:00:00Z</updated>
        <published>2015-12-07T12:00:00Z</published>
        <id>http://sukai.me/advice-for-applying-ml-svm/</id>
        <content type="html">
            <![CDATA[
             <p>ok，我又来继续总结cousera上的机器学习课程了。这次，我主要整理：应用机器学习过程中的几处建议点。本应该还要总结SVM（支持向量机）的，无奈，这部分我暂时还没搞懂，所以，先对这部分留有空白，未来回头在补充。截止本章，所有监督学习的内容差不多就先整理到这了  </p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[神经网络(Neural Networks)基础知识]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/ml-neural-networks/"/>
        <updated>2015-12-03T22:35:00Z</updated>
        <published>2015-12-03T22:35:00Z</published>
        <id>http://sukai.me/ml-neural-networks/</id>
        <content type="html">
            <![CDATA[
             <p>前几天，我们总结了线性回归，逻辑回归等相关的机器学习基础知识，今天我们来总结一下机器学习中一个古老而强大的学习模型：神经网络，主要涉及到：神经网络的模型；工作原理；应用；前向传播和后向传播算法  </p>

<p>那么，开始吧！  </p>

<h3>一. 神经网络模型</h3>

<p>神经网络可以表达一个复杂且非线性的假设模型，其模型表示：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-model.png" width = "680" height = "270" align=center />  </p>

<p>最左边Layer 1称为<strong>输入层</strong>；Layer 2称为<strong>隐藏层</strong>；隐藏层每一个元素也叫做“神经元”，它是由前一层通过“激励函数”计算得来的；最后Layer 3称为<strong>输出层</strong>；该模型隐藏层比较少，绝大多数的数量不止一个  </p>

<p>其中：ai(j)表示第j层的第i个神经元，也称为激励值；激励函数可以选择g(z)=1/(1 + e-z)   </p>

<p>备注：激励函数是对类似的非线性函数g(z)的另一种称呼而已  </p>

<p>那么：  </p>

<ul>
<li>a1(2) = g(Θ10(1)X0 + Θ11(1)x1 + Θ12(1)x2 + Θ13(1)x3)<br/></li>
<li>a2(2) = g(Θ20(1)x0 + Θ21(1)x1 + Θ22(1)x2 + Θ23(1)x3)<br/></li>
<li>a3(2) = g(Θ20(1)x0 + Θ31(1)x1 + Θ32(1)x2 + Θ33(1)x3)<br/></li>
<li>hΘ(x) = a1(3) = g(Θ10(2)a0(2) + Θ11(2)a1(2) + Θ12(2)a2(2) + Θ13(2)a3(2))<br/></li>
</ul>

<p>其中 Θij(l) 是 第l层 第j个单元 与 第l+1层第i个单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）  </p>

<p>ok，一个简单的模型表示出来了，其实还可以精简一点表示：  </p>

<p>重新定义如下：  </p>

<ul>
<li>a1(2) = g(z1(2))<br/></li>
<li>a2(2) = g(z2(2))<br/></li>
<li>a3(2) = g(z3(2))<br/></li>
<li>hΘ(x) = a1(3) = g(z1(3))<br/></li>
<li>z(2) = Θ(1)X<br/></li>
</ul>

<p>其中，z是x0, x1, x2, x3的加权线性组合  </p>

<p>言而总之，我们从输入层的激励开始，然后进行<strong>前向传播</strong>给隐藏层计算，并进行隐藏层的激励，然后继续传播，计算输出层的激励  </p>

<p>所以，神经网络的工作原理是：  </p>

<p>其本质做的就是逻辑回归，但不直接用x1, x2, x3作为输入特征，而是用a1, a2, a3，而a1, a2, a3是由Θ(1)所决定的。即不直接使用输入特征来训练逻辑回归，而是自己训练逻辑回归的输入特征a1, a2, a3  </p>

<p>Θ(1)不同，会训练出不同复杂而有趣的特征  </p>

<h3>二. 神经网络的应用</h3>

<h4>2.1 具体例子</h4>

<p>使用神经网络实现x1 AND x2  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-AND.png" width = "550" height = "370" align=center />  </p>

<p>相应的，可以实现x1 OR x2; x1 XOR x2; x1 XNOR x2。只是，各自的参数权重不同而已  </p>

<h4>2.2 多类型分类系统</h4>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-app.png" width = "680" height = "270" align=center />  </p>

<p>预测的输出值有多个，可以预测多种类型  </p>

<h3>三. 反向传播算法</h3>

<p>神经网络的模型已经被我们表示出来了，那么我们如何学习模型参数？利用反向传播算法  </p>

<p>我们要先找到神经网络的代价函数，目的是min它  </p>

<h4>3.1 神经网络的代价函数</h4>

<p>先定义一些字母变量  </p>

<ul>
<li>L：神经网络中的层数<br/></li>
<li>Sl：第l层神经单元的个数<br/></li>
<li>k：输出单元数量；显然，Binary Classification的k为1，multi-class Classification的k就是k，k维向量<br/></li>
</ul>

<p>既然神经网络的本质就是逻辑回归，我们先来看看逻辑回归的代价函数：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归代价函数总式子.png" width = "630" height = "200" align=center />  </p>

<p>只不过，在神经网络中，输出单元可能不止一个，那么神经网络的代价函数为：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-cost-function.png" width = "630" height = "200" align=center />  </p>

<h4>3.2 反向传播算法</h4>

<p>为了计算导数项，我们将使用反向传播算法  </p>

<p>反向传播算法某种意义上是对每一个结点计算这样的一项：δj(l)，即第l层第j个结点的误差  </p>

<p>具体计算方法：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-1.png" width = "630" height = "300" align=center />  </p>

<p>让我们来整合一下，当我们拥有大量训练样本的时候，如何实现反向传播算法：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-2.png" width = "630" height = "300" align=center />  </p>

<p>好了，仔细看看上面两张图片，我们的最终目的是计算出了偏导数项。其实就是计算出梯度下降的方向  </p>

<p>PS: 我操，步骤好多。参数好多。。  </p>

<h3>四. 实际中如何使用反向传播算法</h3>

<h4>4.1 梯度检查(Gradient Checking)</h4>

<p>梯度检查，是为了避免反向传播算法实现过程中的小错误；当出现错误时，J(Θ)看似在减小，实则最终值会比没有错误时要高  </p>

<p>其核心原理是J(θ)在θ处的偏导数可以约等于(J(θ+ε) - J(θ-ε)) / 2ε，其中ε要足够小，一般去10的-4次方：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查原理.png" width = "630" height = "300" align=center />  </p>

<p>这是高中就知道的知识点了，具体应用到梯度检查中就是：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查算法" width = "630" height = "300" align=center />  </p>

<p>上面图中，我们可以for循环1~n来计算代价函数在每个网络中的参数的偏导数的近似值，记为gradApprox  </p>

<p>我们把反向传播算法计算得到的梯度记为DVec  </p>

<p>这样，我们比较gradApprox和DVec是否近似相等（一般最多几位小数的差距）  </p>

<p>ok，总结一下就是：  </p>

<ul>
<li>先实现反向传播算法，计算出DVec<br/></li>
<li>实现数值梯度检查算法，计算出gradApprox<br/></li>
<li>确定两者给出的值，是接近的<br/></li>
<li>关闭梯度检查（梯度检查计算量相当大），使用反向传播进行神经网络的学习<br/></li>
</ul>

<p>所以，梯度检查是是用来验证你的方向传播的，不是用来学习参数的  </p>

<h4>4.2 如何随机初始化Θ？</h4>

<p>是否可以将Θ的每个参数都初始化为0？不可以！  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/神经网络初始化参数不能为0.png" width = "630" height = "300" align=center />  </p>

<p>这会导致a1(2)永远等于a2(2)；所有的隐藏单元都会通过完全相同的输入函数计算出来，这完全是多余的  </p>

<p>正确的姿势是：打破这种对称的权重  </p>

<p>随机初始化Θij(l)在[-ε, +ε]之间即可，之间的任何一个数  </p>

<h3>五. 总结回顾</h3>

<p>神经网络的知识点还是比较复杂，需要来一波总结  </p>

<h4>5.1 如何选择神经网络的框架</h4>

<p>该选择多少个隐藏层？每个隐藏层拥有多少个神经元？  </p>

<p>一般默认  </p>

<ul>
<li>一个隐藏层<br/></li>
<li>或者多于一个隐藏层，每个隐藏层应具备同样数目的神经元（神经元数量并不是越多越好，越多，计算量也越大），最好能和输入特征的维度匹配<br/></li>
</ul>

<p>比如：3，5，4 或 3，5，5，4 或 3，5，5，5，4  </p>

<h4>5.2 总结神经网络训练模型的步骤</h4>

<p>1.随机初始化权重（即参数）<br/>
2.对于训练集中的每一个x(i)，使用前向传播算法得到hΘ(x(i))<br/>
3.计算代价函数J(Θ)<br/>
4.实现反向传播算法，计算J(Θ)在Θj处的偏导数<br/>
5.使用梯度检查算法，确保反向传播算法无差错工作；之后，关闭该算法<br/>
6.使用梯度下降算法或者其余高级算法（结合反向传播算法），minmizeJ(Θ)，得到最优的模型参数Θ  </p>

<p>反向传播的目的是为了计算梯度下降的方向  </p>

<hr/>

<p>就暂时整理这么多，过几天继续往后整理</p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[微信公众号数据监测项目]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/wx-gsdata/"/>
        <updated>2015-11-29T22:00:00Z</updated>
        <published>2015-11-29T22:00:00Z</published>
        <id>http://sukai.me/wx-gsdata/</id>
        <content type="html">
            <![CDATA[
             <p>最近在给本科实验室的微信监测项目写后台数据源抓取程序，业务逻辑非常简单，在开始编码之前，我设计了一个自认为比较合理（可扩展的）分层。由于数据源是用的一家公司的restful api，使用期间，遇到并解决了很多问题（包括，找出了那家第三方数据公司后台程序和api文档的诸多bug），在这篇博客中记录一下  </p>

<h3>后台设计与编码</h3>

<p>先在这里阐述一下我们项目的数据需求。我们需要监测苏州地区微信公众号的如下信息：  </p>

<ul>
<li>微信公众号的information：公众号描述，联系人名称，公众号所属城市等有关公众号的信息<br/></li>
<li>微信公众号的daily数据：每天的总文章数，总阅读数，总点赞数<br/></li>
<li>每篇文章的daily数据：每天阅读数，点赞数<br/></li>
<li>微信公众号的week数据：一周中的总文章数，总阅读数，总点赞数<br/></li>
<li>每篇文章的week数据：该文章一周的阅读数，点赞数<br/></li>
</ul>

<p>要求就是，从某个api获取这些数据，做持久化入库操作；之后，我们会做一个前台界面（表格展示，图表展示，包括折线图，柱状图等等）方便非技术人员进行监测。前台有关html，js这块不是我编码，我负责数据的来源，下面来看一下我的数据抓取后台分层：  </p>

<p>project package目录：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/wsa.png" width = "200" height = "500" align=center />  </p>

<p>流程图：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/wsa_project.png" width = "790" height = "300" align=center />  </p>

<p>流程图看不清，点击查看大图吧（右击查看图像）&hellip;  </p>

<p>业务逻辑已经被分析彻底了，开始编码吧：  </p>

<p>刷刷刷！！！很快就编码完成（个人觉得，都是体力活，累&hellip;）  </p>

<p>测试：先抓取了5个公众号，10月份31天的数据。没问题，是对的！  </p>

<h3>找到对方api系统bug</h3>

<p>我所写的后台抓取程序，没有涉及到多线程（当时暑假在同程旅游实习，爬大众点评的商品数据，在线程池中运行，一个星期爬了将近百万数据量）。现在的后台程序，就是单条线程不停地运行，请求，存库&hellip;一直循环  </p>

<p>问题来了，对方api系统的bug找出来。具体bug就不说了，最后我汇总了他们的bug和文档bug，和工作人员沟通，他们也很快修复了，这是他们后来回复我的邮件：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/gsdata_bug_reply.png" width = "730" height = "250" align=center />  </p>

<p>ok，截至目前，10个公众号，1月1号～11月23号的数据全部拿出来了，数据库daily表里面有10327条数据  </p>

<h3>数据清理工作：自动化脚本的威力！</h3>

<p>ok，本以为项目可以告一段落了。得知，制作前台界面的小伙伴，调用我后台数据过程中，发现了数据错误现象  </p>

<p>有些棘手了，确实有错误的数据存放在数据库中，但是1w多条，我该怎么排查？  </p>

<p>自动化脚本的威力！！  </p>

<p>我想，编码最大的快乐就是能帮助我们解决琐碎的事，能帮助我们批量解决问题  </p>

<p>刷刷刷！我对10个公众号，1.1 ~ 11.23进行排列组合，循环遍历检查，一共遍历10*11*30（10个公众号，11个月，每个月约30天）  </p>

<p>自动话脚本OK了，运行了8，9mins，结果出来了：  </p>

<p>一共1865条数据有不符合预期的情况，一共分为三种情况：  </p>

<ul>
<li>数据不一致现象：公众号某天的总阅读数，总点赞数，和该公众号该天所有文章的阅读数，点赞数之和不相等<br/></li>
<li>数据库中，部分公众号没有某一天的公众号daily数据<br/></li>
<li>数据库中，部分公众号没有某一天的文章daily数据<br/></li>
</ul>

<p>不符合预期的结果全部被我揪出来了，我该拿它们怎么办？  </p>

<p>自动化脚本的威力：修理他们！！！  </p>

<p>刷刷刷，OK了！  </p>

<p>我们来看一下结果：  </p>

<p>总共1865条不符合预期的数据，经检查：  </p>

<ul>
<li>api bug：api给我的数据就不一致，总共420条<br/></li>
<li>经修理符合要求的数据：39条<br/></li>
<li>公众号确实没有daily数据的，也就是该该公众号当天确实没有推送任何文章：1406条<br/></li>
</ul>

<p>由此可知：我些的后台程序的错误率为：39/1865，或者是39/10327，比较低  </p>

<p>关于那420条api bug返回的数据，算是我发现的另一处bug，我还未和对方沟通  </p>

<p>PS：关于数据清理的工作，我在这篇博客中介绍的比较简单，编写自动化脚本我也一笔带过；但，实则却要理清楚数据清理的逻辑，即：  </p>

<ul>
<li>数据不符合要求的条件是什么？<br/></li>
<li>出现了三种不符合要求的情况，针对这三种，我该怎么修正他们？<br/></li>
<li>如何记录这些错误数据？<br/></li>
</ul>

<p>理不清这些逻辑，呵呵，就更别提写什么脚本帮你处理了。理清之后，脚本刷刷地运行，节省了很多时间  </p>

<p>脚本分别用了python和java语言，期间遇到一些问题，在这里记录一下：  </p>

<ul>
<li><a href="http://stackoverflow.com/questions/4559699/python-mysqldb-and-library-not-loaded-libmysqlclient-16-dylib">python-mysqldb-and-library-not-loaded-libmysqlclient-16-dylib</a><br/></li>
<li><a href="http://blog.csdn.net/dkman803/article/details/1925326">Python MySQLdb 使用utf-8 编码插入中文数据</a><br/></li>
<li><a href="http://zetcode.com/db/mysqlpython/">使用python操作mysql</a><br/></li>
</ul>

<h3>总结</h3>

<h4>第一点：搜索引擎的使用</h4>

<p>强烈建议放弃baidu，拥抱google  </p>

<p>强烈建议放弃baidu，拥抱google  </p>

<p>强烈建议放弃baidu，拥抱google  </p>

<p>重要的事情说三遍！！原因很简单：开发中遇到问题，baidu出来的答案，前几条都是百度知道；google出来的前几条都是stackoverflow。这就是差距  </p>

<h4>第二点：人要学会变得懒惰</h4>

<p>所有程序能帮你完成的事情，我们人为什么还要去做，这简直就是浪费精力  </p>

<p>学好脚本语言，拥有强大的脑洞，生活效率会大大提高  </p>

<h4>第三点：和靠谱的人合作</h4>

<p>找到这家数据公司的诸多文档问题后，我很想吐槽这家数据公司，文档不认真写，感觉有点坑用户  </p>

<p>抛除这些第三方系统的错误或者bug，我想我的工作量很减少很多，我会腾出更多时间去学习其余的知识  </p>

<p>就先写这么多吧，滚去睡觉了！</p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[ML中的逻辑回归与线性回归基础知识]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/ml-cousera-one/"/>
        <updated>2015-11-25T20:00:00Z</updated>
        <published>2015-11-25T20:00:00Z</published>
        <id>http://sukai.me/ml-cousera-one/</id>
        <content type="html">
            <![CDATA[
             <h3>一. 前言</h3>

<p>花了一个多月的时间（中间有段时间在为本科实验室项目写后台程序）在cousera上学习Andrew Ng的机器学习入门公开课，11周的课程马上就要结束了。整体上，我觉得这11周的课程相对比较简单，很容易听懂，少去很多数学的原理推导，很适合初学者，也让初学者建立了机器学习领域的一些宏观概念。之前，很多人推荐李航博士的《统计学习方法》作为入门教程，我个人认为，看完Andrew Ng的课程视频后，再去阅读《统计学习方法》，会轻松很多  </p>

<p>下面我将连载几篇文章，总结这11周课程中的一些内容。主要根据我看视频时，笔记本上纪录的内容为主  </p>

<p>好了，废话不多说。我先总结Week 1~3，主要涉及到监督学习中的线性回归和逻辑回归，避免过拟合等内容  </p>

<h3>二. Week One</h3>

<h4>2.1 Introduction</h4>

<h5>2.1.1 环境的建立</h5>

<p>教你搭建Octave/MATLAB，这部分不多说  </p>

<h5>2.1.2 Introduction</h5>

<p>说明什么是监督学习，什么是非监督学习  </p>

<p>监督学习：  </p>

<ul>
<li>对于离散变量，主要是分类问题<br/></li>
<li>对于连续变量，主要是回归问题<br/></li>
</ul>

<p>非监督学习：数据集没有明确的标签，我们需要在这堆数据中寻找他们的数据结构，常见的：聚类问题  </p>

<h4>2.2 Linear Regression with One Variable</h4>

<p>一元线性回归问题  </p>

<h5>2.2.1 描述一个简单机器学习模型</h5>

<p>首先，说明在一个机器学习模型中，要用到的几个字母变量（符号）的定义：  </p>

<ul>
<li>E: Experience<br/></li>
<li>T: Tasks<br/></li>
<li>P: Performance<br/></li>
<li>x: 特征变量<br/></li>
<li>y: 目标变量<br/></li>
<li>m: 训练集的数目<br/></li>
<li>(x, y): 训练样本<br/></li>
<li>(x(i), y(i)): 第i个训练样本<br/></li>
<li>h: 预测函数<br/></li>
</ul>

<p>好了，有了这些符号，我们就可以表示一个机器学习模型了：  </p>

<p>首先，既然是线程回归问题，我们就先假设预测函数h的公式为：<img src="http://7xl2fd.com1.z0.glb.clouddn.com/预测函数.png" width = "140" height = "25" align=center />  </p>

<p>其中，<img src="http://7xl2fd.com1.z0.glb.clouddn.com/模型参数.png" width = "80" height = "25" align=center />，我们称为模型参数  </p>

<p>然后，学习算法（learning algorithm）会通过training set训练集，得到一个最优的预测函数h（对应一个模型参数）。之后，从测试集中取出x，通过预测函数的映射，即可得到预测的目标变量y  </p>

<p>需要说明的是：如何得到一个最优的预测函数h？  </p>

<p>很简答，就是通过代价函数，寻找使得代价函数最小的模型参数θ  </p>

<p>平方误差是解决回归问题的常用方法，定义一个代价函数J就很简单：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/cost-function.png" width = "300" height = "45" align=center />  </p>

<p>通过某种方法，寻找到使得代价函数最小的那个模型参数θ，即是我们最终需要的结果  </p>

<h5>2.2.2 如何求解模型参数θ（专业地说：如何进行参数学习）</h5>

<p>cousera上介绍了一种最经典的算法：<strong>梯度下降算法</strong>  </p>

<p>大致的工作过程即，从某个随机的θ0, θ1开始，不停地改变θ0, θ1的值，以保证J(θ0, θ1)不断地减小，直至在某个最小值处结束（不停地在迭代运行）  </p>

<p>来张图理解一下（仿佛一个人下山的过程）  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/gradient%20descent%20img.png" width = "700" height = "360" align=center />  </p>

<p><strong>梯度下降算法定义：</strong>  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/gradient%20descent%20define.png" width = "400" height = "100" align=center />  </p>

<p>其中，α叫做学习速率（learning rate），α很大，下山的步伐会迈的很大；α很小，下山的步伐会迈的相对较小。α用以控制我们以多大的幅度去更新参数θj  </p>

<p>α不宜设置过大，否则容易出现波动，难以收敛  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法中的学习速率.png" width = "700" height = "360" align=center />  </p>

<p>再看看α后面的偏导数项，它代表什么意思呢？  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法中的偏导数.png" width = "700" height = "380" align=center />  </p>

<p>先看上面的图，偏导数表示红色线的斜率，此时斜率为正，那么θ1就会减去α*一个正数，θ1会逐渐变小，直至最低点  </p>

<p>下面的图，斜率为负，θ1会逐渐变大，直至最低点  </p>

<p>ok，梯度下降算法定义解释完毕。在本例中，hθ(x)=θ0+θ1*x，那么梯度下降算法的具体步骤为：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/一元线性回归的梯度下降算法.png" width = "430" height = "180" align=center />  </p>

<p>此时，已经具体到了θ0和θ1的具体变换过程  </p>

<p>需要注意的是：θ0和θ1是需要同时更新变化执行迭代的  </p>

<h3>三. Week Two</h3>

<h4>3.1 Linear Regression with Multiple Variables</h4>

<p>多元线性回归问题  </p>

<p>既然已经不止一个特征变量了，那么之前用x表示特征变量，现在要换一种表示方法了：  </p>

<ul>
<li>n: 表示特征值的个数<br/></li>
<li>xj(i)：第i个训练样本的第j个特征的值<br/></li>
</ul>

<p>此时预测函数h表示为：hθ(x) = θT*x  </p>

<p>为了符号的方便表示，我们总是定义x(j)0为1  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的预测函数.png" width = "550" height = "280" align=center />  </p>

<p>下面，解释在多元线性回归中的梯度下降算法：  </p>

<p>首先，是它的代价函数  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的代价函数.png" width = "520" height = "110" align=center />  </p>

<p>然后是θj的变换过程  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/多元线性回归的梯度下降算法.png" width = "450" height = "280" align=center />  </p>

<p>ok，下面再介绍梯度下降算法在实际应用中的两个注意点：  </p>

<ul>
<li>特征缩放<br/></li>
<li>学习速率的选择<br/></li>
</ul>

<p><strong>特征缩放：</strong>  </p>

<p>假设，现有一个房价预测系统，若现有两个特征：x1, x2。x1表示房屋的面积，取值范围：0~2000，x2表示房间的个数，取值范围：1~5  </p>

<p>这两个特征的取值的取值规模不在一个级别，使用梯度下降算法时，收敛不快，且容易发生波动。应使用特征缩放方法将两个特征的取值范围约束到大体一致的范围：  </p>

<ul>
<li>一种方法是：x1=x1/2000，每个特征处以该特征值的最大值。这样，取值范围都被缩小为0~1。一般-1~1, -1/3~1/3和-3~3均可接受<br/></li>
<li>第二种方法，归一化处理：（xi-平均值）/（MAX-MIN）<br/></li>
</ul>

<p><strong>学习速率α</strong>  </p>

<p>上面已经解释了learning rate的意义。正常的梯度下降算法，随着迭代次数的增加，代价函数逐渐减小；不正常的工作流程，有可能出现波动现象  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度下降算法的迭代过程.png" width = "440" height = "300" align=center />  </p>

<p>不正常的工作现象：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/不正常的梯度下降过程.png" width = "510" height = "300" align=center />  </p>

<p>看了公开课后的结论是：α一般从0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1开始选择  </p>

<p>截至目前，我们一直都在使用梯度下降算法求解最优的模型参数。梯度下降算法最显著的特征就是，需要不停地迭代  </p>

<p>视频中还介绍了另一种求解方法：<strong>正规方程（Normal Equation）</strong>  </p>

<p>θ=(xT*x)-1*xT*y（证明略&hellip;）  </p>

<p>它一步即可得到最优的模型参数值  </p>

<p>现在，我们来比较一下：  </p>

<table border="1">  
<tr>  
<th>梯度下降算法（Gradient Descent）</th>  
<th>正规方程（Normal Equation）</th>  
</tr>  
<tr>  
<td>需要选择学习速率α</td>  
<td>不需要选择学习速率α</td>  
</tr>  
<tr>  
<td>需要多次迭代</td>  
<td>不需要迭代，一步得结果</td>  
</tr>  
<tr>  
<td>当特征数目n特别大时，也可以正常工作</td>  
<td>当特征数目n特别大时，矩阵纬度很大，计算非常慢（n=100,1000还好，n为千万时，计算非常慢）</td>  
</tr>  
</table>  
  

<h3>四. Week Three 逻辑回归</h3>

<h4>4.1 分类问题</h4>

<p>先从两元分类问题进行研究：  </p>

<p>y∈{0, 1}，定义y=0时，属于Negtive Class；y＝1时，属于Positive Class  </p>

<p>那么，怎样让预测函数 0 &lt;= hθ(x) &lt;= 1？  </p>

<p>这里介绍S型函数g(z)=1/(1 + e-z)  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/S型函数.png" width = "510" height = "300" align=center />  </p>

<p>S型函数的取值范围在(0, 1)，则hθ(x) = 1/（1 + e-(θT*x)）；而且hθ(x) = P(y=1 | x;θ) = 1 - P(y=0 | x;θ)  </p>

<ul>
<li>当θT*x &gt;= 0，hθ(x) &gt;= 0.5，y为1<br/></li>
<li>当θT*x &lt; 0，hθ(x) &lt; 0.5，y为0<br/></li>
</ul>

<p>举例：hθ(x) = -3 + x1 + x2；当-3 + x1 + x2 &gt;= 0，预测y=1；此时决策边界为 x1 + x2 = 3  </p>

<p>需要注意的是：我们不是用训练集来定义决策边界的，而是用来拟合参数θ的，θ决定了决策边界  </p>

<h4>4.2 代价函数和梯度下降算法</h4>

<p>之前在线性回归中定义的平方误差代价函数，是否能用到逻辑回归中呢？  </p>

<p>答案是：不能  </p>

<p>因为逻辑回归中的预测函数hθ(x)是非线性函数，得到的代价函数将是非凸函数，可能是这样：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/非凸代价函数.png" width = "410" height = "250" align=center />  </p>

<p>它有许多局部最优值，梯度下降算法不能保证收敛到全局最优值。所以，我们希望代价函数是有类似于“单弓形”函数，能确保找到全局最小值  </p>

<p>我们需要重新定义逻辑回归的代价函数了  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归的代价函数.png" width = "490" height = "150" align=center />  </p>

<p>当 y = 1，hθ(x) -&gt; 0, cost -&gt; ∞  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/y=1的逻辑回归代价函数.png" width = "410" height = "250" align=center />  </p>

<p>当 y = 0，hθ(x) -&gt; 1, cost -&gt; ∞  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/y=0的逻辑回归代价函数.png" width = "410" height = "250" align=center />  </p>

<p>ok，新定义的代价函数，符合我们的要求  </p>

<p>此时的代价函数公式，用一行表示为：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归代价函数总式子.png" width = "630" height = "200" align=center />  </p>

<p>使用梯度下降算法，迭代并同时更新θ0～θn  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归梯度下降算法.png" width = "530" height = "200" align=center />  </p>

<p>和线性回归的一样，只不过hθ(x)的表达式不同  </p>

<p>当然，特征缩放也是适用于逻辑回归的  </p>

<h4>4.3 高级算法</h4>

<p>除了梯度下降，正规方程，还有许多可以计算出最优模型参数θ的算法  </p>

<p>视频中还提到了：BFGS, L-BFGS和Conjugate gradient  </p>

<p>这些高级算法比梯度下降要快，无需选择学习速率，但是更复杂。具体编程时，可以考虑使用现成的类库，帮助我们快速解决问题  </p>

<h4>4.4 多元分类问题</h4>

<p>三元分类问题  </p>

<p>可以转换为三个两元分类问题来做  </p>

<p>hθ(i)(x) = P(y = i | x:θ)，其中 i＝1，2，3  </p>

<p>训练的出θ后，预测时，选择hθ(i)(x)最大的那个i，就是目标变量的分类  </p>

<h4>4.5 过拟合问题</h4>

<h5>4.5.1 什么是过拟合？什么是欠拟合？</h5>

<ul>
<li>欠拟合：预测模型没有能很好的拟合训练数据，拟合效果差。产生高偏差<br/></li>
<li>过拟合：能很好地拟合训练数据。但预测函数变量过多，而没有足够多的数据去约束变量的个数（预测测试集，效果会非常差，即泛化效果差）。产生高方差<br/></li>
</ul>

<p><a href="http://www.zhihu.com/question/20448464">高偏差；高仿差..click for infos</a>  </p>

<p>＊泛化：一个测试模型应用到新样本的能力  </p>

<p>这里，先不讲如何识别过拟合问题，后面的课程讲解了如果利用工具去识别过拟合问题（后续整理）  </p>

<p>往往，我们往预测模型中，增加了太多太多的特征，容易导致过拟合问题  </p>

<p>如何避免过拟合问题：  </p>

<ul>
<li>减少特征变量数目（缺点：同时也会丢失一些重要信息）<br/></li>
<li>正则化（进行某种惩罚措施）：保留所有的特征变量，但是降低模型参数θj的数量级或者值<br/></li>
</ul>

<p>正则化技术非常有效，由于是在我们预测模型中有许多特征变量，每个特征变量都能帮助我们预测目标变量，这样我们就可以不用舍弃任何一个特征变量了  </p>

<h5>4.5.2 线性回归代价函数中使用正则化技术：</h5>

<p>重写线性回归中的代价函数：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/正则化的线性回归代价函数.png" width = "530" height = "150" align=center />  </p>

<p>λ为正则化参数。λ越大，惩罚程度越大，模型参数θj会越小，有可能会导致欠拟合问题  </p>

<h5>4.5.3 线性回归梯度下降算法中使用正则化技术：</h5>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/正则化的梯度下降算法.png" width = "650" height = "400" align=center />  </p>

<p>注意θ0是单独的一种  </p>

<h5>4.5.4 正规方程中使用正则化技术：</h5>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/正则化的正规方程.png" width = "380" height = "180" align=center />  </p>

<h5>4.5.5 逻辑回归代价函数中使用正则化技术：</h5>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归正则化代价函数.png" width = "730" height = "300" align=center />  </p>

<h5>4.5.6 逻辑回归梯度下降算法中使用正则化技术：</h5>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归正则化梯度下降算法.png" width = "730" height = "350" align=center />  </p>

<p>这个和线性回归中的一样，hθ(x)不同而已  </p>

<h5>4.5.6 高级算法中使用正则化技术：</h5>

<p>略  </p>

<h3>五. 结语</h3>

<p>ok，前三章整理完了。过几天整理神经网络那块  </p>

<p>感谢七牛云存储为文章提供图片CDN支持  </p>

<p>文章若有错误，请指正</p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[自顶向下学习Tornado源码(一)]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/tornado-source-code-one/"/>
        <updated>2015-11-21T10:00:00Z</updated>
        <published>2015-11-21T10:00:00Z</published>
        <id>http://sukai.me/tornado-source-code-one/</id>
        <content type="html">
            <![CDATA[
             <h3>一. 从Hello Tornado开始</h3>

<h4>1.1 Hello World</h4>

<p>用tornado写的一个最简单的demo，但已经用到了很多tornado的核心类：httpserver, ioloop, Application&hellip;  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/hello-tornado.png" width = "500" height = "400" alt="tornado-demo" align=center />  </p>

<h4>1.2 调试运行</h4>

<p>利用pdb进行调试，打印方法栈信息，在IndexHandler中加入代码：  </p>
<div class="highlight"><pre><span class="n">greeting</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_argument</span><span class="p">(</span><span class="s">&#39;greeting&#39;</span><span class="p">,</span> <span class="s">&#39;Hello&#39;</span><span class="p">)</span>  
<span class="kn">import</span> <span class="nn">pdb</span>  
<span class="n">pdb</span><span class="o">.</span><span class="n">set_trace</span><span class="p">()</span>   
<span class="bp">self</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">greeting</span> <span class="o">+</span> <span class="s">&#39;, friendly user!&#39;</span><span class="p">)</span>
</pre></div>

<p>利用curl或者浏览器，触发程序，查看栈桢  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/stack-hello-tornado.png" width = "500" height = "400" alt="stack-tornado-demo" align=center />  </p>

<h4>1.3 猜测</h4>

<p>图中模模糊糊给了我们一些执行流程信息，我又参考了几篇文章，先得出一些结论  </p>

<ul>
<li>ioloop：核心的io循环，用于处理所有socket的read, write, accept等事件。其i/o模型视主机而定，一般Linux平台采用epoll(关于i/o模型，<a href="http://sukai.me/linux-five-io-models/">Click For More..</a>)<br/></li>
<li>iostream：封装了对socket的异步读写操作<br/></li>
<li>httpConnection(代码在httpserver.py中)：接受HTTP请求，根据路由信息，调用相应的Handler。再把相应的数据写会client<br/></li>
</ul>

<p>tornado服务器的大体流程如下图(图片来源：<a href="http://kenby.iteye.com/blog/1159621">kenby.iteye</a>)  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/tornado-core.png" width = "500" height = "400" alt="tornado-core" align=center />  </p>

<p>首先，服务器创建监听套接字，同时将自己注册进ioloop进行事件循环检查（主要检查是否有新的客户端连接到来）。client到达后，服务器通过accept函数，返回一个client socket，同时client socket被注册进ioloop进行事件循环检查（检查read，write等事件）。客户端若向服务器发起读操作，HTTPConnetcion通过_on_headers解析HTTP头，之后通过iostream的write操作，将数据写给客户端  </p>

<h3>二. HTTPServer</h3>

<p>按照自顶向下的顺序，先阅读HTTPServer的源码  </p>

<p>HTTPServer继承自TCPServer  </p>

<p>刚刚的demo中，与HTTPServer相关的只有两条编程语句  </p>
<div class="highlight"><pre><span class="n">http_server</span> <span class="o">=</span> <span class="n">tornado</span><span class="o">.</span><span class="n">httpserver</span><span class="o">.</span><span class="n">HTTPServer</span><span class="p">(</span><span class="n">app</span><span class="p">)</span>  
<span class="n">http_server</span><span class="o">.</span><span class="n">listen</span><span class="p">(</span><span class="n">options</span><span class="o">.</span><span class="n">port</span><span class="p">)</span>
</pre></div>

<p>先是初始化函数  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/tornado-httpserver-init.png" width = "600" height = "400" alt="tornado-httpserver-init" align=center />  </p>

<p>就是简单地初始化一些参数，然后就是父类的初始化。最主要的是这个request_callback，我们传进去的是Application实例，里面有我们自己配制的路由信息，到时候tornado服务器会根据我们的路由信息，将相应的路由请求转发给相应的RequestHandler。  </p>

<p>然后是<code>listen</code>监听，HTTPServer没有覆写父类TCPServer的listen函数，直接看TCPServer的listen：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/tornado-tcpsever-listen.png" width = "500" height = "200" alt="tornado-tcpserver-listen" align=center />  </p>

<p>先调用了<code>bind_socket</code>，该函数在netutil.py中：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/netutil-bind_socket.png" width = "550" height = "100" alt="netutil-bind_socket" align=center />  </p>

<p>代码就不看了，功能很简单，我截图了该函数的doc string，根据给定的地址和端口，创建一系列的listening sockets  </p>

<p>之后<code>listen</code>函数调用了<code>self.add_sockets(sockets)</code>，我们来看一下<code>add_sockets</code>：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/tcpserver-add_sockets.png" width = "650" height = "320" alt="tornado-tcpserver-add_sockets" align=center />  </p>

<p>通过<code>add_accept_handler(sock, self._handle_connection, io_ioop=elf.io_loop)</code>将一系列的listen sockets注册进ioloop（ioloop的作用就是利用epoll，轮询检查所有套接字的io事件，效率很高，是tornado并发量突出的重要原因）  </p>

<p>仔细看一下<code>add_accept_handler()</code>，它在netuilt.py中：  </p>
<div class="highlight"><pre><span class="k">if</span> <span class="n">io_loop</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  
    <span class="n">io_loop</span> <span class="o">=</span> <span class="n">IOLoop</span><span class="o">.</span><span class="n">current</span><span class="p">()</span>  

<span class="k">def</span> <span class="nf">accept_handler</span><span class="p">(</span><span class="n">fd</span><span class="p">,</span> <span class="n">events</span><span class="p">):</span>  
    <span class="c"># More connections may come in while we&#39;re handling callbacks;  </span>
    <span class="c"># to prevent starvation of other tasks we must limit the number  </span>
    <span class="c"># of connections we accept at a time.  Ideally we would accept  </span>
    <span class="c"># up to the number of connections that were waiting when we  </span>
    <span class="c"># entered this method, but this information is not available  </span>
    <span class="c"># (and rearranging this method to call accept() as many times  </span>
    <span class="c"># as possible before running any callbacks would have adverse  </span>
    <span class="c"># effects on load balancing in multiprocess configurations).  </span>
    <span class="c"># Instead, we use the (default) listen backlog as a rough  </span>
    <span class="c"># heuristic for the number of connections we can reasonably  </span>
    <span class="c"># accept at once.  </span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">_DEFAULT_BACKLOG</span><span class="p">):</span>  
        <span class="k">try</span><span class="p">:</span>  
            <span class="n">connection</span><span class="p">,</span> <span class="n">address</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">accept</span><span class="p">()</span>  
        <span class="k">except</span> <span class="n">socket</span><span class="o">.</span><span class="n">error</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  
            <span class="c"># _ERRNO_WOULDBLOCK indicate we have accepted every  </span>
            <span class="c"># connection that is available.  </span>
            <span class="k">if</span> <span class="n">errno_from_exception</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_ERRNO_WOULDBLOCK</span><span class="p">:</span>  
                <span class="k">return</span>  
            <span class="c"># ECONNABORTED indicates that there was a connection  </span>
            <span class="c"># but it was closed while still in the accept queue.  </span>
            <span class="c"># (observed on FreeBSD).  </span>
            <span class="k">if</span> <span class="n">errno_from_exception</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">==</span> <span class="n">errno</span><span class="o">.</span><span class="n">ECONNABORTED</span><span class="p">:</span>  
                <span class="k">continue</span>  
            <span class="k">raise</span>  
        <span class="n">callback</span><span class="p">(</span><span class="n">connection</span><span class="p">,</span> <span class="n">address</span><span class="p">)</span>  
<span class="n">io_loop</span><span class="o">.</span><span class="n">add_handler</span><span class="p">(</span><span class="n">sock</span><span class="p">,</span> <span class="n">accept_handler</span><span class="p">,</span> <span class="n">IOLoop</span><span class="o">.</span><span class="n">READ</span><span class="p">)</span>
</pre></div>

<p>利用<code>io_loop.add_handler(sock, accept_handler, IOLoop.READ)</code>，将listen socket注册进ioloop，等待READ事件发生。发生后即回调<code>accept_handler</code>函数，该函数里面有<code>sock.accept()</code>方法，接受客户端连接，之后再调用<code>callback(connection, address)</code>。这里的callback函数就是我们调用<code>add_accept_handler()</code>时候，传进去的<code>self._handle_connection</code>，它的代码在<code>TCPServer</code>中，核心的功能代码内容是  </p>
<div class="highlight"><pre><span class="k">try</span><span class="p">:</span>  
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ssl_options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  
        <span class="n">stream</span> <span class="o">=</span> <span class="n">SSLIOStream</span><span class="p">(</span><span class="n">connection</span><span class="p">,</span> <span class="n">io_loop</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">io_loop</span><span class="p">,</span>  
                <span class="n">max_buffer_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_buffer_size</span><span class="p">,</span>  
                <span class="n">read_chunk_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">read_chunk_size</span><span class="p">)</span>  
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">stream</span> <span class="o">=</span> <span class="n">IOStream</span><span class="p">(</span><span class="n">connection</span><span class="p">,</span> <span class="n">io_loop</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">io_loop</span><span class="p">,</span>  
                <span class="n">max_buffer_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_buffer_size</span><span class="p">,</span>  
                <span class="n">read_chunk_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">read_chunk_size</span><span class="p">)</span>  
    <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handle_stream</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="n">address</span><span class="p">)</span>  
    <span class="k">if</span> <span class="n">future</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">io_loop</span><span class="o">.</span><span class="n">add_future</span><span class="p">(</span><span class="n">future</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">f</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">result</span><span class="p">())</span>  
    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>  
        <span class="n">app_log</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s">&quot;Error in connection callback&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>

<p>为每个connection创建<code>IOStream</code>实例（用于对socket的异步读写），之后的IO操作由此实例负责（其实在IOStream中的读写事件也会注册到ioloop中，后续分析）。然后调用<code>self.handle_stream(stream, address)</code>，<code>handle_stream()</code>由HTTPServer覆写：  </p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">handle_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stream</span><span class="p">,</span> <span class="n">address</span><span class="p">):</span>  
    <span class="n">context</span> <span class="o">=</span> <span class="n">_HTTPRequestContext</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="n">address</span><span class="p">,</span>  
                        <span class="bp">self</span><span class="o">.</span><span class="n">protocol</span><span class="p">)</span>  
    <span class="n">conn</span> <span class="o">=</span> <span class="n">HTTP1ServerConnection</span><span class="p">(</span>  
            <span class="n">stream</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">conn_params</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>  
    <span class="bp">self</span><span class="o">.</span><span class="n">_connections</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>  
    <span class="n">conn</span><span class="o">.</span><span class="n">start_serving</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>

<p><code>handler_stream</code>创建了HTTPConnection对象，HTTPConnection对象负责处理剩下的交互部分  </p>

<p>ok，HTTPServer大体的核心流程走完了一遍  </p>

<h3>三. 后续</h3>

<p>现在，所有的socket（listen和client）都被注册进了ioloop，它们会被不断地轮询检查，是否有io事件发生。如有读写事件发生，会通过iostream封装的异步READ和WRITE函数进行。HTTPConnection，IOLOOP和IOSTERAM后续再进行分析  </p>

<hr/>

<p>本文参考：<a href="http://www.douban.com/group/topic/13276435/">mywaiting.douban</a>; <a href="kenby.iteye.com/blog/1159621">kenby.iteye</a></p>

            ]]>
        </content>
    </entry>
    
</feed>