<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>苏苏</title>
    <link href="" rel="self" />
    <link href="http://sukai.me/" />
    <updated>2016-01-05T14:00:00Z</updated>
    <id>http://sukai.me/</id>
    
    <entry>
        <title><![CDATA[Shadowsocks科学上网]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/shadowsocks/"/>
        <updated>2016-01-05T14:00:00Z</updated>
        <published>2016-01-05T14:00:00Z</published>
        <id>http://sukai.me/shadowsocks/</id>
        <content type="html">
            <![CDATA[
             <p>GFW，想必大家都不陌生  </p>

<p>针对GFW，渴求真理和自由的程序员们绞尽脑汁，创造出了许多梯子工具。本人在大二的时候尝试翻墙，用过免费VPN，用过GoAgent等等，这些都不算是自食其力式梯子，直到后来了解到了Shadowsocks，提到Shadowsocks，大家一定不陌生这句话：<strong>“Removed according to regulations.”</strong>  </p>

<p>当然，我在写这篇文章之前，已经使用Shadowsocks快一年了，见证了ss从未封杀到被封杀。写这篇文章，是因为：  </p>

<ul>
<li>1.翻墙对我很重要，我要将“Shadowsocks科学上网”的方案备份下来，我的生活已离不开翻墙<br/></li>
<li>2.这几天很多人问我借ss账号，我有必要告诉他人如何科学上网<br/></li>
</ul>

<p>申明：扯了那么多，我其实并不会讲如何利用shadowsocks科学上网，因为这些教程随意Google一大堆（当然，Baidu是不可以的，因为根据相关法律&hellip;&hellip;）。可能你们又会说，我既然不能翻墙，我怎么google，这不是一个死循环吗？ok，我直接贴出一篇教程出来：  </p>

<p><a href="http://www.findspace.name/res/956">FindSpace：SHADOWSOCKS科学上网</a>  </p>

<p>FindeSpace是我的博客友链，我当时也是因为搜索到了Findspace的教程，搞定了ss，和他交了朋友  </p>

<p>所以，当时，按照他的教程：  </p>

<p>我在host1plus上买了一个最最便宜的vps（美国洛杉矶节点），一个月10+RMB，然后在vps上装了ubuntu系统，ssh上去安装了python和shadowsocks（pip install），最后编写了json格式的配置文件，并使用nohup ssserver命令在后台启动服务。 最后在本地也同样安装shadowsocks，以及配置配置文件，使用sslocal启动服务。 我是为了翻墙才买的vps，所以，最终用firefox代理插件，配置本地的shadowsocks代理端口，即可科学上网了（现在换了MAC，不需要任何代理插件了，也不需要命令行启动ss客户端了）。 需要说明的是，ios和android都是有shadowsocks客户端的，所以，科学上网不仅限于PC机端。还有，买个vps跑个shadowsocks，未免太浪费了，还可以跑跑其他程序的，很多人都用vps建站搭博客等。 前前后后不到一小时搞定  </p>

<p>再给大家列举一些资源列表：  </p>

<ul>
<li><a href="https://github.com/Long-live-shadowsocks">long-live-shadowsocks</a>：仅仅用来备份 shadowsocks 相关项目（re-uploaded because I can）<br/></li>
</ul>

<p>这个仓库很强大，ios，android，mac，win的ss客户端都有，快去下载吧  </p>

<p>Best Wishes!!</p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[再见，2015！你好，2016]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/bye-2015-hello-2016/"/>
        <updated>2015-12-26T15:00:00Z</updated>
        <published>2015-12-26T15:00:00Z</published>
        <id>http://sukai.me/bye-2015-hello-2016/</id>
        <content type="html">
            <![CDATA[
             <h3>一. 再见，2015</h3>

<h4>生病</h4>

<p>我的2015年是从生病开始的，准确的说，我的2014年是以生病结束的。2014年11月底，身上莫名其妙地出现许多出血点，经过两三家医院的确认，是过敏性紫癜  </p>

<p>印象中，从2014.11~2015.4，一直在喝中药，因为过敏性紫癜的西医治方就是强的松等激素，我一开始尝试了一个多月，效果很快，但治标不治本，很容易反弹。所以，2015年寒假，我第一次没有留校做项目，回家尝试中药治疗  </p>

<p>总而言之，我觉得：刚生病那会，及时的药物治疗非常重要，到了后期，药物治疗为辅，心态治疗为主，一个好的心态才是根本  </p>

<p>最近也经常看到网络上各种程序猿猝死的新闻，想在这里提醒广大同行的程序猿：爱惜自己  </p>

<h4>实习面试</h4>

<p>2015年4~6月那会，周围的同学都在忙着寻找暑期实习。我投的公司不多，期初只投了知名的阿里和腾讯。两者全部止步在了笔试，前者参加了笔试，后面并没有收到面试通知；后者我直接放弃了笔试。最后，投了同程旅游，拿到了暑期实习offer。在我拿到同程旅游的offer之后，收到了360的内推邀请，那时，我也做出了保研的决定，也没有尝试360，因为我知道我的水平有限，而且我的暑期肯定会选择一个离苏州最近的实习单位，因为我还要忙保研的事情  </p>

<h4>实习</h4>

<p>2015.7.13左右，加入同程旅游，工号已经到了17000多，那是我觉得自己最有钱的一段时间，每天200的工资，而且同程旅游办公地点的对面就是我们苏州大学独墅湖校区，所以不必去公司楼下的快餐店吃昂贵的午餐，可以去食堂吃，而且同程旅游给实习生安排了免费的住宿  </p>

<p>当时加入同程旅游的还有我的其余一些小伙伴，我在那边度过了一段有钱且开心的日子  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/ly-03.jpg" alt=""/>  </p>

<p>当时和我合作的是比我大一届的正式员工，他叫杨鑫，感谢他给我的一些指导和帮助，离职的时候，我也加了他的微信  </p>

<h4>保研</h4>

<p>2015.8从同程旅游离职，8~9月参加了2个学校的保研面试，2015.10月中旬，提交了保研志愿，保研结束了，最终保送到了东南大学计算机科学与工程学院，研究方向：AI  </p>

<h4>保研之后</h4>

<ul>
<li>在coursera上学完吴恩达的ML公开课，开始入门机器学习<br/></li>
<li>再为本科实验室一个项目写了一个后台程序<br/></li>
<li>开始刷题<br/></li>
<li>玩玩玩<br/></li>
</ul>

<h3>二. 你好，2016</h3>

<p>再过几天，元旦了，又将迎来崭新的一年  </p>

<p>生活和工作都不能稀里糊涂，2016年里，我先给自己定一些清单：  </p>

<ul>
<li>毕设：认认真真完成毕设，这是我在研究生实验室的首秀，希望别搞砸，争取省级优秀<br/></li>
<li>读书：2015年，我读了64本书；2016年争取达到李开复老师所说的一年100本，每天再忙也要在多看上读至少50页书<br/></li>
<li>算法：多刷题，多看算法，就先从leetcode开始<br/></li>
<li>paper：论文就先从导师给的tutorial开始<br/></li>
<li>公开课：多看ted，主要涉及cs这块，而非编程语言和软件开发这块<br/></li>
<li>英语：拒绝百度，使用google搜索高质量英文文章，可以用英文写写段子（每天背单词？这个我做不到）<br/></li>
<li>锻炼：一步一步恢复体育锻炼，找回曾经那个阳光帅气的自己（过敏性紫癜不能剧烈运动）<br/></li>
<li>体重：2016年争取到120斤<br/></li>
<li>效率：熟练使用mac，vim，shell，Alfred。熟练使用神器，能帮助你提高工作效率<br/></li>
<li>其余目标视具体项目或者课程情况而定，拥抱变化<br/></li>
<li>继续保持着积极乐观，提高自控力，多思考，多成长<br/></li>
</ul>

<p>暂且这么多，希望自己能完成  </p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[用Python实现线性回归]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/python-linear-regerssion/"/>
        <updated>2015-12-10T18:00:00Z</updated>
        <published>2015-12-10T18:00:00Z</published>
        <id>http://sukai.me/python-linear-regerssion/</id>
        <content type="html">
            <![CDATA[
             <h3>为什么用Python？</h3>

<p><strong>“</strong>  </p>

<p>说起科学计算，首先会被提到的可能就是强大的MATLAB。然而除了MATLAB的一些专业性很强的工具箱目前还无法替代外，MATLAB的大部分常用功能都可以在Python世界中找到相应的扩展库。和MATLAB相比，用Python做科学计算有如下优点：  </p>

<ul>
<li>免费<br/></li>
<li>更易学<br/></li>
<li>丰富扩展库<br/></li>
</ul>

<p><strong>”</strong>  </p>

<p>&mdash; 这段话摘自<a href="http://book.douban.com/subject/7175280/">《Python科学计算》</a>  </p>

<p>在这篇文章中，我就利用了Python的NumPy库，帮助我更快实现了线性回归中的一些内容：  </p>

<p>代码中注释很详细，直接阅读代码吧：  </p>
<div class="highlight"><pre><span class="c">#!/usr/bin/env python  </span>
<span class="c"># -*- coding: utf-8 -*-  </span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>  

<span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">Implement Linear-Regression, using Python  </span>
<span class="sd">&#39;&#39;&#39;</span>  


<span class="k">def</span> <span class="nf">loadDataSet</span><span class="p">():</span>  
    <span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">    Read data from file  </span>
<span class="sd">    Return:  </span>
<span class="sd">        x:list, [[x0(0), x1(0)], [x0(1), x1(1)] ... [x0(m), x1(m)]]  </span>
<span class="sd">        y:list, [y(0), y(1), ... y(m)]  </span>
<span class="sd">    &#39;&#39;&#39;</span>  
    <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>  
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>  
    <span class="n">dataFile</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;ex1data1.txt&#39;</span><span class="p">)</span>  
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">dataFile</span><span class="p">:</span>  
        <span class="n">lineData</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;,&#39;</span><span class="p">)</span>  
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">lineData</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>  
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">lineData</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>  
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  


<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  
    <span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">    Hypothesis Function For one sample  </span>
<span class="sd">        theta: 个数为n的一维ndarray  </span>
<span class="sd">        x: 个数为n的一维ndarray  </span>
<span class="sd">    Return: digit  </span>
<span class="sd">    &#39;&#39;&#39;</span>  
    <span class="k">return</span> <span class="n">theta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  


<span class="k">def</span> <span class="nf">batch_gradient_descent</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  
    <span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">    Batch-Gradient-Descent&quot;  </span>
<span class="sd">        alpha: Learning Rate  </span>
<span class="sd">        x: list, [[x0(0), x1(0)], [x0(1), x1(1)] ... [x0(m), x1(m)]]  </span>
<span class="sd">        y: list, [y(0), y(1), ... y(m)]  </span>
<span class="sd">        theta: 默认为np.array([0]*n, dtype=np.float)  </span>
<span class="sd">    Return:  </span>
<span class="sd">        newTheta: 训练后的模型参数，个数为n的一维ndarray  </span>
<span class="sd">    &#39;&#39;&#39;</span>  
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  
    <span class="n">newTheta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>  
        <span class="n">count</span> <span class="o">=</span> <span class="mf">0.0</span>  
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>  
            <span class="c"># x[i,:]取x第i行，形成n个元素的一维矩阵  </span>
            <span class="n">count</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>  
        <span class="n">newTheta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">count</span> <span class="o">/</span> <span class="n">m</span>  
    <span class="k">return</span> <span class="n">newTheta</span>  


<span class="k">def</span> <span class="nf">normal_equation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  
    <span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">    Normal Equation  </span>
<span class="sd">    &#39;&#39;&#39;</span>  
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  


<span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>  
    <span class="sd">&quot;&quot;&quot;  </span>
<span class="sd">    Cost Function  </span>
<span class="sd">        theta: 模型参数，个数为n的一维ndarray  </span>
<span class="sd">        x: m*n的二维ndarray  </span>
<span class="sd">        y: 个数为m的一维ndarray  </span>
<span class="sd">        x.dot(theta): 个数为m的一维矩阵  </span>
<span class="sd">    &quot;&quot;&quot;</span>  
    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span>  


<span class="k">def</span> <span class="nf">test</span><span class="p">():</span>  
    <span class="sd">&#39;&#39;&#39;  </span>
<span class="sd">    Test Function  </span>
<span class="sd">    &#39;&#39;&#39;</span>  
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">loadDataSet</span><span class="p">()</span>  
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>  
    <span class="k">for</span> <span class="n">iters</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>  
        <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost_function</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>  
        <span class="n">theta</span> <span class="o">=</span> <span class="n">batch_gradient_descent</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
    <span class="k">print</span> <span class="s">&#39;Batch-Gradient-Descent:&#39;</span><span class="p">,</span> <span class="s">&#39;</span><span class="se">\n</span><span class="s">cost:</span><span class="se">\n</span><span class="s">&#39;</span><span class="p">,</span> <span class="n">costs</span>  
    <span class="k">print</span> <span class="s">&#39;theta: &#39;</span><span class="p">,</span> <span class="n">theta</span>  
    <span class="k">print</span> <span class="s">&#39;Hypothesis: &#39;</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.4994</span><span class="p">])),</span> <span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span>  

    <span class="k">print</span> <span class="s">&#39;Normal-Equation:&#39;</span>  
    <span class="n">theta</span> <span class="o">=</span> <span class="n">normal_equation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
    <span class="k">print</span> <span class="s">&#39;theta: &#39;</span><span class="p">,</span> <span class="n">theta</span>  
    <span class="k">print</span> <span class="s">&#39;Hypothesis: &#39;</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.4994</span><span class="p">]))</span>  


<span class="k">if</span> <span class="n">__name__</span><span class="o">==</span><span class="s">&#39;__main__&#39;</span><span class="p">:</span>  
    <span class="n">test</span><span class="p">()</span>
</pre></div>

<hr/>

<p>参考：  </p>

<ul>
<li><a href="http://zhouyichu.com/machine-learning/Gradient-Code.html">梯度下降法的Python实现</a><br/></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html">Numpy.Quickstart tutorial</a><br/></li>
<li>《机器学习实战》<br/></li>
</ul>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[Machine Learning应用的几处建议]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/advice-for-applying-ml-svm/"/>
        <updated>2015-12-07T16:30:00Z</updated>
        <published>2015-12-07T16:30:00Z</published>
        <id>http://sukai.me/advice-for-applying-ml-svm/</id>
        <content type="html">
            <![CDATA[
             <p>ok，我又来继续总结cousera上的机器学习课程了。这次，我主要整理：应用机器学习过程中的几处建议点。本应该还要总结SVM（支持向量机）的，无奈，这部分我暂时还没搞懂，所以，先对这部分留有空白，未来回头在补充。截止本章，所有监督学习的内容差不多就先整理到这了  </p>

<h3>一. 机器学习应用的一些建议</h3>

<h4>1.1 如何评价一个学习算法的好坏？</h4>

<p>当训练得到的模型参数泛化能力很弱的时候，下一步该做什么？  </p>

<p>很多人认为 1. 增加训练样本，即可改进算法，所以把时间浪费在了收集样本上。通常，盲目地扩大训练样本于事无补  </p>

<p>除了增加训练样本外，还有一些其余的改进方法：  </p>

<p>2.尝试选择更少的特征集，防止过拟合<br/>
3.尝试选择更多的特征集，防止欠拟合<br/>
4.增加多项式特征，如x1-&gt;x1的平方<br/>
5.减少或者增加正则化参数λ  </p>

<p>当我们的模型参数泛化能力差时，这1~5中改进方法，我们选择哪一种？  </p>

<p>通过机器学习诊断算法去帮助我们决定，下面会介绍  </p>

<h4>1.2 如何评价假设函数？</h4>

<p><strong>A. 如何判断一个假设函数是过拟合的？</strong>  </p>

<p>我们先来介绍一个概念：Test Error（测试误差）  </p>

<p>我们将所有样本按照7：3，分为：Training Set和Test Set  </p>

<p>线性回归中，我们可以利用训练集得到的学习参数θ，去计算测试集的代价函数Jtest(θ)，作为测试误差：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/线性回归中的测试误差.png" width = "600" height = "200" align=center />  </p>

<p>其中，mtest表示测试样本数量；(xtest(i), ytest(i))表示第i个测试样本  </p>

<p>逻辑回归中，同样的道理，测试误差为：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归的测试误差.png" width = "720" height = "200" align=center />  </p>

<p>除此之外，逻辑回归中还有一种更加直观的，用来表示测试误差的方式：误分类率，也叫0/1错分率  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归的误分类率.png" width = "740" height = "300" align=center />  </p>

<p>其中，err(hθ(x), y)有两种值：  </p>

<ul>
<li>当hθ(x) &gt;= 0.5，y = 0；或者 hθ(x) &lt;= 0.5，y = 1时候，err值为1。这时都是没有正确预测的情况<br/></li>
<li>其余情况，正确预测，err的值为0<br/></li>
</ul>

<p>此时的测试误差，就是测试集合的err总和的均值  </p>

<p>这样，我们就可以通过<strong>测试误差</strong>去评价我们学习出来的预测函数（假设函数）了，也可以看出假设函数是否能很好的拟合测试数据  </p>

<h4>1.3 模型选择和 训练/交叉验证/测试 集合</h4>

<p>假设d表示我们要选择的多项式的次数，取值为[1, 10]，我们最终该选择何种模型，即d的取值选择多少，才能使得过拟合和欠拟合适中？  </p>

<p>我们可以取d的值为1~10，10个值都分别去训练学习，得到相应取值d下的最优学习参数θ(1) &hellip; θ(10)，然后计算测试误差，得到Jtest(hθ(1)，Jtest(hθ(2)) &hellip; Jtest(hθ(10))  </p>

<p>取最小的Jtest(hθ(i))，i为d的最优取值  </p>

<p>我们用测试误差，去拟合d的值。但，这也不能确切地保证，假设函数在新样本中的泛化能力。我们需要改进我们的集合划分策略，将数据集分为三部分：  </p>

<ul>
<li>Training Set 60%<br/></li>
<li>Cross Validation Set 20%<br/></li>
<li>Test Set 20%<br/></li>
</ul>

<p>我们可以在三中样本上，定义三种误差，方法上面已经讲过：  </p>

<ul>
<li>Training Error<br/></li>
<li>Cross Validation Error<br/></li>
<li>Test Error<br/></li>
</ul>

<p>我们的多项式次数选择方法，改进如下：  </p>

<p>我们转向去计算cv集上的误差：Jcv(hθ(1)，Jcv(hθ(2)) &hellip; Jcv(hθ(10))，取交叉测试集误差最小的d，比如d=4。最后在计算Jtest(θ(4))，来判断其与测试集有没有拟合  </p>

<p>这样，可以回避测试集的嫌疑  </p>

<h3>二. 偏差 VS 方差</h3>

<h4>2.1 学会诊断偏差和方差</h4>

<p>学习参数泛化不理想，无外乎两种情况：欠拟合和过拟合，前者对应着较大的偏差，后者对应着较大的方差，我们需要学会诊断什么是高方差，什么是高偏差，这样，才能帮助我们进行学习的改进  </p>

<p>继续上面选择多项式次数d的例子，我们绘制出图形如下，横坐标为d，纵坐标为error：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/error-d图.png" width = "700" height = "400" align=center />  </p>

<p>我们可以从图片中猜测，偏向左端的d值对应着欠拟合问题，偏向右端的d值对应着过拟合问题：  </p>

<p>欠拟合出现，会产生高偏差：  </p>

<ul>
<li>Jtrain(θ) will be high<br/></li>
<li>Jcv(θ) 约等于 Jtrain(θ)<br/></li>
</ul>

<p>过拟合出现，会产生高方差：  </p>

<ul>
<li>Jtrain(θ) will be low<br/></li>
<li>Jcv(θ) &gt;&gt; Jtrain(θ) 远大于<br/></li>
</ul>

<p>这也是我们区别高偏差和高方差的方法  </p>

<h4>2.2 正则化与偏差和方差</h4>

<p>众所周知，正则化技术用来防止过拟合现象  </p>

<p>正则化参数λ过大，模型参数θ将被大大惩罚，结果会导致θ值近似于0，并且h(θ)值也将趋近于0，出现欠拟合现象  </p>

<p>正则化参数λ过小，会出现过拟合情况  </p>

<p><strong>如何选择何止的参数λ？</strong>  </p>

<p>按照之前遵循的方法，取λ值为0, 0.01, 0.02 &hellip; 10.24（一切我们想尝试的λ值），利用train set训练出相应λ值下得模型参数θ。之后，我们就可以用cv集评价假设函数和参数了，选择cv集误差最小的参数λ作为结果  </p>

<p>最终，我们会得到这样的曲线图：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/λ和error曲线图.png" width = "600" height = "330" align=center />  </p>

<p>我们可以根据绘制出的曲线图，选择合适的参数λ  </p>

<h4>2.3 学习曲线</h4>

<p>绘制学习曲线，有助于检查学习算法是否运行正常  </p>

<p>正常情况下得学习曲线：Jtrain(0)随着训练样本数量m的增加，逐渐增大；而Jcv(0)逐渐减小  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/正常情况下得学习曲线.png" width = "600" height = "330" align=center />  </p>

<p>欠拟合情况下，随着m增加，训练集误差和cv集误差趋近于相等，两者预测效果非常接近，都不理想：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/欠拟合的学习曲线.png" width = "600" height = "330" align=center />  </p>

<p>此时，即使你增加训练样本的数目m，基本上不会起作用，因为cv集误差或者测试误差随着m增加，趋近于直线  </p>

<p>过拟合情况下：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/过拟合的学习曲线.png" width = "600" height = "330" align=center />  </p>

<p>此时，获取更多的样本容量，是起作用的：蓝色和红色曲线会慢慢靠近  </p>

<h4>2.4 下一步，我们该做什么？</h4>

<p>整理完前面的知识，一开始就介绍的五种改进学习算法的方法，它们对应的使用情况就明白了：  </p>

<p>1.增加训练样本（fix high variance）<br/>
2.尝试选择更少的特征集，防止过拟合（fix high variance）<br/>
3.尝试选择更多的特征集，防止欠拟合（fix high bias）<br/>
4.增加多项式特征，如x1-&gt;x1的平方（fix high bias）<br/>
5.减少或者增加正则化参数λ（fix high bias/variance）  </p>

<h3>三. 实战：机器学习系统设计</h3>

<h4>3.1 误差分析</h4>

<p>推荐的一种误差分析方法：  </p>

<p>1.先从一种简单的学习算法入手，一种你能很快实现的算法。然后在cv集中测试该学习算法<br/>
2.绘制学习曲线，决定：增加样本容量；增减特征；等等是否有用<br/>
3.误差分析：手动检查那些cv集中出错的样本，发现系统性规律：那些样本是出错的，是否存在共性等等  </p>

<h4>3.2 处理偏态数据</h4>

<p>何为偏态数据（skewed data）：一个类的样本数比另一个类的样本数多很多  </p>

<p>此时，假设函数如果总是预测y=0或者y=1，利用cv error评价该假设函数，会得到不错的评价结果  </p>

<p>所以，我们针对偏态数据，必须寻找新的度量标准：  </p>

<p>这里，我们将介绍Precision和Recall，查准率和召回率  </p>

<p>首先，看图表：  </p>

<table border="1">  
<tr>  
<th></th>  
<th>样本实际值为1</th>  
<th>| 样本实际值为0</th>  
</tr>  
<tr>  
<td>预测值为1：</td>  
<td>true positive</td>  
<td>| flase positive</td>  
</tr>  
<tr>  
<td>预测值为0：</td>  
<td>false negativie</td>  
<td>| true negative</td>  
</tr>  
</table>  
  

<p>基于上述图表，  </p>

<ul>
<li>查准率定义为：(True positive) / (Predict positive) = (True positive) / (True positive + False positive)，我们预测属于1分类的样本中，有多大比率，我们是正确<br/></li>
<li>召回率定义为：(True positive) / (Actual positive) = (True positive) / (True positive + False negative)，样本中确实属于1分类的样本，有多大比率，我们正确地预测了<br/></li>
</ul>

<p>很容易理解  </p>

<h4>3.3 权衡查准率和召回率</h4>

<p>以预测病人是否得癌症为例：  </p>

<p>当我们十分十分确定该病人患有癌症时，我们才预测y=1，此时满足：Higher Precision, lower Recall  </p>

<p>当我们不想错过任何一个可能患有癌症的病人时，满足：Higher Recall, lower Precision  </p>

<h4>3.4 如何比较Recall（R） / Precision（P）？</h4>

<p>定义F1 = (2 * PR) / (P+R)  </p>

<h3>四. SVM：支持向量机</h3>

<p>略  </p>

            ]]>
        </content>
    </entry>
    
    <entry>
        <title><![CDATA[神经网络(Neural Networks)基础知识]]></title>
        <author><name>SuKai</name><uri>http://sukai.me/</uri></author>
        <link href="http://sukai.me/ml-neural-networks/"/>
        <updated>2015-12-03T22:35:00Z</updated>
        <published>2015-12-03T22:35:00Z</published>
        <id>http://sukai.me/ml-neural-networks/</id>
        <content type="html">
            <![CDATA[
             <p>前几天，我们总结了线性回归，逻辑回归等相关的机器学习基础知识，今天我们来总结一下机器学习中一个古老而强大的学习模型：神经网络，主要涉及到：神经网络的模型；工作原理；应用；前向传播和后向传播算法  </p>

<p>那么，开始吧！  </p>

<h3>一. 神经网络模型</h3>

<p>神经网络可以表达一个复杂且非线性的假设模型，其模型表示：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-model.png" width = "680" height = "270" align=center />  </p>

<p>最左边Layer 1称为<strong>输入层</strong>；Layer 2称为<strong>隐藏层</strong>；隐藏层每一个元素也叫做“神经元”，它是由前一层通过“激励函数”计算得来的；最后Layer 3称为<strong>输出层</strong>；该模型隐藏层比较少，绝大多数的数量不止一个  </p>

<p>其中：ai(j)表示第j层的第i个神经元，也称为激励值；激励函数可以选择g(z)=1/(1 + e-z)   </p>

<p>备注：激励函数是对类似的非线性函数g(z)的另一种称呼而已  </p>

<p>那么：  </p>

<ul>
<li>a1(2) = g(Θ10(1)X0 + Θ11(1)x1 + Θ12(1)x2 + Θ13(1)x3)<br/></li>
<li>a2(2) = g(Θ20(1)x0 + Θ21(1)x1 + Θ22(1)x2 + Θ23(1)x3)<br/></li>
<li>a3(2) = g(Θ20(1)x0 + Θ31(1)x1 + Θ32(1)x2 + Θ33(1)x3)<br/></li>
<li>hΘ(x) = a1(3) = g(Θ10(2)a0(2) + Θ11(2)a1(2) + Θ12(2)a2(2) + Θ13(2)a3(2))<br/></li>
</ul>

<p>其中 Θij(l) 是 第l层 第j个单元 与 第l+1层第i个单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）  </p>

<p>ok，一个简单的模型表示出来了，其实还可以精简一点表示：  </p>

<p>重新定义如下：  </p>

<ul>
<li>a1(2) = g(z1(2))<br/></li>
<li>a2(2) = g(z2(2))<br/></li>
<li>a3(2) = g(z3(2))<br/></li>
<li>hΘ(x) = a1(3) = g(z1(3))<br/></li>
<li>z(2) = Θ(1)X<br/></li>
</ul>

<p>其中，z是x0, x1, x2, x3的加权线性组合  </p>

<p>言而总之，我们从输入层的激励开始，然后进行<strong>前向传播</strong>给隐藏层计算，并进行隐藏层的激励，然后继续传播，计算输出层的激励  </p>

<p>所以，神经网络的工作原理是：  </p>

<p>其本质做的就是逻辑回归，但不直接用x1, x2, x3作为输入特征，而是用a1, a2, a3，而a1, a2, a3是由Θ(1)所决定的。即不直接使用输入特征来训练逻辑回归，而是自己训练逻辑回归的输入特征a1, a2, a3  </p>

<p>Θ(1)不同，会训练出不同复杂而有趣的特征  </p>

<h3>二. 神经网络的应用</h3>

<h4>2.1 具体例子</h4>

<p>使用神经网络实现x1 AND x2  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-AND.png" width = "550" height = "370" align=center />  </p>

<p>相应的，可以实现x1 OR x2; x1 XOR x2; x1 XNOR x2。只是，各自的参数权重不同而已  </p>

<h4>2.2 多类型分类系统</h4>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-app.png" width = "680" height = "270" align=center />  </p>

<p>预测的输出值有多个，可以预测多种类型  </p>

<h3>三. 反向传播算法</h3>

<p>神经网络的模型已经被我们表示出来了，那么我们如何学习模型参数？利用反向传播算法  </p>

<p>我们要先找到神经网络的代价函数，目的是min它  </p>

<h4>3.1 神经网络的代价函数</h4>

<p>先定义一些字母变量  </p>

<ul>
<li>L：神经网络中的层数<br/></li>
<li>Sl：第l层神经单元的个数<br/></li>
<li>k：输出单元数量；显然，Binary Classification的k为1，multi-class Classification的k就是k，k维向量<br/></li>
</ul>

<p>既然神经网络的本质就是逻辑回归，我们先来看看逻辑回归的代价函数：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/逻辑回归代价函数总式子.png" width = "630" height = "200" align=center />  </p>

<p>只不过，在神经网络中，输出单元可能不止一个，那么神经网络的代价函数为：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/neural-network-cost-function.png" width = "630" height = "200" align=center />  </p>

<h4>3.2 反向传播算法</h4>

<p>为了计算导数项，我们将使用反向传播算法  </p>

<p>反向传播算法某种意义上是对每一个结点计算这样的一项：δj(l)，即第l层第j个结点的误差  </p>

<p>具体计算方法：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-1.png" width = "630" height = "300" align=center />  </p>

<p>让我们来整合一下，当我们拥有大量训练样本的时候，如何实现反向传播算法：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/反向传播算法-2.png" width = "630" height = "300" align=center />  </p>

<p>好了，仔细看看上面两张图片，我们的最终目的是计算出了偏导数项。其实就是计算出梯度下降的方向  </p>

<p>PS: 我操，步骤好多。参数好多。。  </p>

<h3>四. 实际中如何使用反向传播算法</h3>

<h4>4.1 梯度检查(Gradient Checking)</h4>

<p>梯度检查，是为了避免反向传播算法实现过程中的小错误；当出现错误时，J(Θ)看似在减小，实则最终值会比没有错误时要高  </p>

<p>其核心原理是J(θ)在θ处的偏导数可以约等于(J(θ+ε) - J(θ-ε)) / 2ε，其中ε要足够小，一般去10的-4次方：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查原理.png" width = "630" height = "300" align=center />  </p>

<p>这是高中就知道的知识点了，具体应用到梯度检查中就是：  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/梯度检查算法" width = "630" height = "300" align=center />  </p>

<p>上面图中，我们可以for循环1~n来计算代价函数在每个网络中的参数的偏导数的近似值，记为gradApprox  </p>

<p>我们把反向传播算法计算得到的梯度记为DVec  </p>

<p>这样，我们比较gradApprox和DVec是否近似相等（一般最多几位小数的差距）  </p>

<p>ok，总结一下就是：  </p>

<ul>
<li>先实现反向传播算法，计算出DVec<br/></li>
<li>实现数值梯度检查算法，计算出gradApprox<br/></li>
<li>确定两者给出的值，是接近的<br/></li>
<li>关闭梯度检查（梯度检查计算量相当大），使用反向传播进行神经网络的学习<br/></li>
</ul>

<p>所以，梯度检查是是用来验证你的方向传播的，不是用来学习参数的  </p>

<h4>4.2 如何随机初始化Θ？</h4>

<p>是否可以将Θ的每个参数都初始化为0？不可以！  </p>

<p><img src="http://7xl2fd.com1.z0.glb.clouddn.com/神经网络初始化参数不能为0.png" width = "630" height = "300" align=center />  </p>

<p>这会导致a1(2)永远等于a2(2)；所有的隐藏单元都会通过完全相同的输入函数计算出来，这完全是多余的  </p>

<p>正确的姿势是：打破这种对称的权重  </p>

<p>随机初始化Θij(l)在[-ε, +ε]之间即可，之间的任何一个数  </p>

<h3>五. 总结回顾</h3>

<p>神经网络的知识点还是比较复杂，需要来一波总结  </p>

<h4>5.1 如何选择神经网络的框架</h4>

<p>该选择多少个隐藏层？每个隐藏层拥有多少个神经元？  </p>

<p>一般默认  </p>

<ul>
<li>一个隐藏层<br/></li>
<li>或者多于一个隐藏层，每个隐藏层应具备同样数目的神经元（神经元数量并不是越多越好，越多，计算量也越大），最好能和输入特征的维度匹配<br/></li>
</ul>

<p>比如：3，5，4 或 3，5，5，4 或 3，5，5，5，4  </p>

<h4>5.2 总结神经网络训练模型的步骤</h4>

<p>1.随机初始化权重（即参数）<br/>
2.对于训练集中的每一个x(i)，使用前向传播算法得到hΘ(x(i))<br/>
3.计算代价函数J(Θ)<br/>
4.实现反向传播算法，计算J(Θ)在Θj处的偏导数<br/>
5.使用梯度检查算法，确保反向传播算法无差错工作；之后，关闭该算法<br/>
6.使用梯度下降算法或者其余高级算法（结合反向传播算法），minmizeJ(Θ)，得到最优的模型参数Θ  </p>

<p>反向传播的目的是为了计算梯度下降的方向  </p>

<hr/>

<p>就暂时整理这么多，过几天继续往后整理</p>

            ]]>
        </content>
    </entry>
    
</feed>